import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# T√©l√©charger les ressources n√©cessaires
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)
nltk.download('vader_lexicon', quiet=True)

# 1. Charger le dataset
df = pd.read_csv("Avis_Client.csv", sep=",", encoding="utf-8", quotechar='"', engine="python")

print("=" * 60)
print("APER√áU DES DONN√âES")
print("=" * 60)
print(df.head())
print(f"\nNombre total d'avis : {len(df)}")

# 2. CR√âER DE VRAIS LABELS AVEC VADER (analyse de sentiment)
print("\n" + "=" * 60)
print("CR√âATION DES LABELS AVEC VADER")
print("=" * 60)

sia = SentimentIntensityAnalyzer()


def get_sentiment_vader(text):
    """Utilise VADER pour d√©terminer le sentiment"""
    if not isinstance(text, str):
        return 'neutre'

    scores = sia.polarity_scores(text)
    compound = scores['compound']

    # Classification bas√©e sur le score compound
    if compound >= 0.05:
        return 'positif'
    elif compound <= -0.05:
        return 'negatif'
    else:
        return 'neutre'


# G√©n√©rer les labels r√©els
df['Sentiment_reel'] = df['Avis'].apply(get_sentiment_vader)

print("\nDistribution des sentiments r√©els :")
print(df['Sentiment_reel'].value_counts())
print("\nPourcentage :")
print(df['Sentiment_reel'].value_counts(normalize=True) * 100)

# 3. Pr√©traitement du texte AM√âLIOR√â
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Ajouter des mots-cl√©s de sentiment fran√ßais (si vos avis sont en fran√ßais)
mots_positifs_fr = {'bon', 'excellent', 'super', 'g√©nial', 'parfait', 'satisfait',
                    'content', 'merveilleux', 'top', 'recommande'}
mots_negatifs_fr = {'mauvais', 'nul', 'horrible', 'd√©cevant', 'm√©diocre',
                    'insatisfait', 'probl√®me', 'arnaque', '√©viter'}


def clean_text_advanced(text):
    if not isinstance(text, str):
        return ""

    text_lower = text.lower()

    # Conserver certains patterns importants avant nettoyage
    text_lower = re.sub(r"pas bon|pas bien|pas satisfait", "negatif_expression", text_lower)
    text_lower = re.sub(r"tr√®s bon|tr√®s bien|tr√®s satisfait", "positif_expression", text_lower)

    # Nettoyage standard
    text_lower = re.sub(r"http\S+|www\S+|https\S+", '', text_lower)
    text_lower = re.sub(r'\d+', '', text_lower)
    text_lower = text_lower.translate(str.maketrans('', '', string.punctuation))

    # Tokenization et lemmatisation
    words = text_lower.split()

    # Ne pas supprimer les stopwords n√©gatifs importants
    important_words = {'not', 'no', 'never', 'neither', 'nobody', 'nothing',
                       'pas', 'jamais', 'rien', 'aucun'}
    words = [word for word in words if word not in stop_words or word in important_words]
    words = [lemmatizer.lemmatize(word) for word in words]

    return " ".join(words)


df['clean_avis'] = df['Avis'].apply(clean_text_advanced)

print("\n" + "=" * 60)
print("EXEMPLES DE TEXTE NETTOY√â")
print("=" * 60)
print(df[['Avis', 'clean_avis', 'Sentiment_reel']].head(10))

# 4. Pr√©paration des donn√©es
X = df['clean_avis']
y = df['Sentiment_reel']  # ‚≠ê UTILISER LES VRAIS LABELS

# V√©rifier s'il y a assez de donn√©es par classe
print(f"\nDistribution des classes :\n{y.value_counts()}")

# Split stratifi√© pour garder la m√™me proportion dans train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

print(f"\nTaille du train : {len(X_train)}")
print(f"Taille du test : {len(X_test)}")

# 5. TF-IDF avec param√®tres optimis√©s
vectorizer = TfidfVectorizer(
    max_features=2000,  # R√©duit pour √©viter l'overfitting avec peu de donn√©es
    ngram_range=(1, 3),  # Inclure les trigrammes
    min_df=2,  # Ignorer les mots qui apparaissent dans moins de 2 documents
    max_df=0.8,  # Ignorer les mots trop fr√©quents
    sublinear_tf=True  # Utiliser le log du TF
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

print(f"\nDimensions de la matrice TF-IDF : {X_train_vec.shape}")

# 6. GESTION DU D√âS√âQUILIBRE DES CLASSES avec SMOTE
print("\n" + "=" * 60)
print("APPLICATION DE SMOTE POUR √âQUILIBRER LES CLASSES")
print("=" * 60)

try:
    smote = SMOTE(random_state=42, k_neighbors=min(5, min(y_train.value_counts()) - 1))
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_vec, y_train)
    print("SMOTE appliqu√© avec succ√®s")
    print(f"Distribution apr√®s SMOTE :\n{pd.Series(y_train_balanced).value_counts()}")
except Exception as e:
    print(f" SMOTE non applicable : {e}")
    print("On continue sans SMOTE...")
    X_train_balanced, y_train_balanced = X_train_vec, y_train

# 7. ENTRA√éNEMENT DE PLUSIEURS MOD√àLES
print("\n" + "=" * 60)
print("ENTRA√éNEMENT DES MOD√àLES")
print("=" * 60)

models = {
    'Logistic Regression': LogisticRegression(
        max_iter=1000,
        random_state=42,
        C=10,  # R√©gularisation plus forte
        class_weight='balanced'  #  Important pour classes d√©s√©quilibr√©es
    ),
    'Naive Bayes': MultinomialNB(alpha=0.1),
    'Random Forest': RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        random_state=42,
        class_weight='balanced'
    )
}

results = {}

for name, model in models.items():
    print(f"\n Entra√Ænement : {name}")
    model.fit(X_train_balanced, y_train_balanced)
    y_pred = model.predict(X_test_vec)

    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy

    print(f"Accuracy : {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"\nRapport de classification :")
    print(classification_report(y_test, y_pred, zero_division=0))
    print(f"\nMatrice de confusion :")
    print(confusion_matrix(y_test, y_pred))

# 8. S√âLECTIONNER LE MEILLEUR MOD√àLE
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]

print("\n" + "=" * 60)
print(f" MEILLEUR MOD√àLE : {best_model_name}")
print(f" Accuracy : {results[best_model_name]:.4f}")
print("=" * 60)

# 9. Pr√©diction sur tous les avis
X_all_vec = vectorizer.transform(X)
df['Sentiment_predit'] = best_model.predict(X_all_vec)

# 10. Comparaison VADER vs Mod√®le
print("\n" + "=" * 60)
print("COMPARAISON DES PR√âDICTIONS")
print("=" * 60)

print("\nSentiments VADER (labels d'entra√Ænement) :")
print(df['Sentiment_reel'].value_counts(normalize=True) * 100)

print("\nSentiments pr√©dits par le mod√®le :")
print(df['Sentiment_predit'].value_counts(normalize=True) * 100)

# 11. Aper√ßu final
print("\n" + "=" * 60)
print("APER√áU FINAL DES PR√âDICTIONS")
print("=" * 60)
print(df[['Avis', 'Sentiment_reel', 'Sentiment_predit']].head(20))

# 12. Analyse des erreurs
print("\n" + "=" * 60)
print("ANALYSE DES ERREURS")
print("=" * 60)

errors = df[df['Sentiment_reel'] != df['Sentiment_predit']]
print(f"\nNombre d'erreurs : {len(errors)} / {len(df)} ({len(errors) / len(df) * 100:.2f}%)")

if len(errors) > 0:
    print("\nExemples d'erreurs :")
    print(errors[['Avis', 'Sentiment_reel', 'Sentiment_predit']].head(10))

# 13. Sauvegarder les r√©sultats
df.to_csv("Avis_Client_avec_sentiments.csv", index=False, encoding='utf-8')
print("\n R√©sultats sauvegard√©s dans 'Avis_Client_avec_sentiments.csv'")

# 14. CONSEILS POUR AM√âLIORER DAVANTAGE
print("\n" + "=" * 60)
print("üí° RECOMMANDATIONS POUR AM√âLIORER LE MOD√àLE")
print("=" * 60)
print("""
1.  LABELLISER MANUELLEMENT : Avec seulement 100 avis, la meilleure 
   approche est de cr√©er un fichier Excel avec une colonne 'Sentiment' 
   que vous remplissez manuellement (positif/negatif/neutre).

2.  LANGUE : Si vos avis sont en fran√ßais, utilisez :
   - stopwords.words('french') au lieu de 'english'
   - Un mod√®le pr√©-entra√Æn√© fran√ßais (CamemBERT, FlauBERT)

3.  PLUS DE DONN√âES : 100 avis est tr√®s peu. Essayez d'en collecter plus.

4.  MOD√àLES AVANC√âS : Utilisez des transformers pr√©-entra√Æn√©s :
   - from transformers import pipeline
   - classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')

5.  CLASSES D√âS√âQUILIBR√âES : Si une classe domine, utilisez :
   - class_weight='balanced' (d√©j√† fait)
   - SMOTE pour sur-√©chantillonner (d√©j√† fait)
""")